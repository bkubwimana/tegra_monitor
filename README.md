# VLLM MMLU Benchmark

A comprehensive MMLU evaluation system edge systems with perf and energy instrumentation.

## ğŸš€ Features

- **VLLM Integration**: High-performance inference with comprehensive metrics
- **Parameter Sweeping**: Automated model and token limit sweeps
- **Three Evaluation Modes**: Base (reasoning), Budget (efficient), NoReasoning (direct)
- **Telemetry Monitoring**: Real-time performance and energy tracking
- **Answer Extraction**: Multi-pattern choice extraction with confidence scoring
- **Configuration-Driven**: YAML-based evaluation configurations

## ğŸ“ Project Structure

```
vllm-mmlu-benchmark/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/              
â”‚   â”‚   â””â”€â”€ vllm_model.py    
â”‚   â”œâ”€â”€ evaluators/          
â”‚   â”‚   â”œâ”€â”€ base_evaluator.py        
â”‚   â”‚   â”œâ”€â”€ budget_evaluator.py      
â”‚   â”‚   â””â”€â”€ noreasoning_evaluator.py 
â”‚   â”œâ”€â”€ telemetry/           
â”‚   â”‚   â””â”€â”€ monitor.py       
â”‚   â”œâ”€â”€ data_loaders/            
â”‚   â”‚   â””â”€â”€ mmlu_loader.py   
â”‚   â””â”€â”€ utils/               
â”‚       â””â”€â”€ answer_extraction.py 
â”œâ”€â”€ configs/                 
â”‚   â”œâ”€â”€ base.yaml           
â”‚   â”œâ”€â”€ budget.yaml         
â”‚   â””â”€â”€ noreasoning.yaml    
â”œâ”€â”€ tests/                  
â”œâ”€â”€ all_budget.py           
â”œâ”€â”€ sweep_budget.sh         
â””â”€â”€ README.md              
```

## ğŸ› ï¸ Installation

```bash
pip install vllm transformers datasets pyyaml tqdm
```

## ğŸ“Š Usage

### Parameter Sweeping

```bash
# Run automated sweep across models and token limits
./sweep_budget.sh

# Single evaluation
python3 all_budget.py --model "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" --max-tokens 256
```

### Configuration

Edit `sweep_budget.sh` to customize:

```bash
MODELS=(
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
    "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
)

MAX_TOKENS_VALUES=(128 256 512)
```

## âš™ï¸ Evaluation Modes

**Base**: Full reasoning with 4096 tokens
**Budget**: Efficient evaluation with configurable token limits (128-512)
**NoReasoning**: Direct answer selection with 4096 tokens

## ğŸ“ˆ Output Files

### Summary Results (`all_subjects_summary.json`)
```json
{
  "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
  "overall_accuracy": 0.319,
  "total_questions": 3000,
  "total_correct": 959,
  "config_details": {
    "model_settings": {
      "max_tokens": 128,
      "temperature": 0.6
    }
  }
}
```

### Performance Metrics (`detailed_results_*.csv`)
Per-question results with timing, token counts, and accuracy data.

### Telemetry Data (`tegrastats_*.log`, `energy_*.csv`)
System performance and energy consumption metrics.

## ğŸ”¬ Supported Models

- `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`
- `deepseek-ai/DeepSeek-R1-Distill-Qwen-14B`
- `deepseek-ai/DeepSeek-R1-Distill-Llama-8B`
- `l3lab/L1-Qwen-1.5B-Max`
- Custom models via command line

## ğŸ§ª Testing

```bash
python tests/test_answer_extraction.py
python tests/test_mmlu_loader.py
```

## ï¿½ Architecture

- **VLLM Integration**: Native API with performance monitoring
- **Modular Design**: Pluggable evaluators and configurations  
- **Telemetry System**: Real-time energy and performance tracking
- **Answer Extraction**: Multi-pattern matching with confidence scoring

---

**Comprehensive MMLU benchmarking with perf and energy metrics**
